{
  if (filterTableType == Functions.FilterTableType.HDFS) {
    logger.info("Start to load HDFS filter table from " + filterResourceIdentifier);
    Stopwatch stopwatch=new Stopwatch().start();
    FileSystem fileSystem=null;
    try {
      fileSystem=FileSystem.get(HBaseConfiguration.create());
      long modificationTime=fileSystem.getFileStatus(new Path(filterResourceIdentifier)).getModificationTime();
      Pair<Long,Set<ByteArray>> cached=hdfs_caches.getIfPresent(filterResourceIdentifier);
      if (cached != null && cached.getFirst().equals(modificationTime)) {
        ret=cached.getSecond();
        logger.info("Load HDFS from cache using " + stopwatch.elapsedMillis() + " millis");
        return;
      }
      InputStream inputStream=fileSystem.open(new Path(filterResourceIdentifier));
      List<String> lines=IOUtils.readLines(inputStream);
      logger.info("Load HDFS finished after " + stopwatch.elapsedMillis() + " millis");
      for (      String line : lines) {
        if (StringUtils.isEmpty(line)) {
          continue;
        }
        try {
          ByteArray byteArray=ByteArray.allocate(encoding.getLengthOfEncoding());
          encoding.encode(line.getBytes(),line.getBytes().length,byteArray.array(),0);
          ret.add(byteArray);
        }
 catch (        Exception e) {
          logger.warn("Error when encoding the filter line " + line);
        }
      }
      hdfs_caches.put(filterResourceIdentifier,Pair.newPair(modificationTime,ret));
      logger.info("Mass In values constructed after " + stopwatch.elapsedMillis() + " millis, containing "+ ret.size()+ " entries");
    }
 catch (    IOException e) {
      throw new RuntimeException("error when loading the mass in values",e);
    }
  }
 else {
    throw new RuntimeException("HBASE_TABLE FilterTableType Not supported yet");
  }
}
