{
  ResourceStore rs=ResourceStore.getStore(kylinConfig);
  String fileKey=cubeSegment.getStatisticsResourcePath();
  InputStream is=rs.getResource(fileKey);
  File tempFile=null;
  FileOutputStream tempFileStream=null;
  try {
    tempFile=File.createTempFile(cubeSegment.getUuid(),".seq");
    tempFileStream=new FileOutputStream(tempFile);
    org.apache.commons.io.IOUtils.copy(is,tempFileStream);
  }
  finally {
    IOUtils.closeStream(is);
    IOUtils.closeStream(tempFileStream);
  }
  Map<Long,Long> cuboidSizeMap=Maps.newHashMap();
  FileSystem fs=HadoopUtil.getFileSystem("file:///" + tempFile.getAbsolutePath());
  SequenceFile.Reader reader=null;
  try {
    reader=new SequenceFile.Reader(fs,new Path(tempFile.getAbsolutePath()),conf);
    LongWritable key=(LongWritable)ReflectionUtils.newInstance(reader.getKeyClass(),conf);
    BytesWritable value=(BytesWritable)ReflectionUtils.newInstance(reader.getValueClass(),conf);
    int samplingPercentage=25;
    while (reader.next(key,value)) {
      if (key.get() == 0l) {
        samplingPercentage=Bytes.toInt(value.getBytes());
      }
 else {
        HyperLogLogPlusCounter hll=new HyperLogLogPlusCounter(14);
        ByteArray byteArray=new ByteArray(value.getBytes());
        hll.readRegisters(byteArray.asBuffer());
        cuboidSizeMap.put(key.get(),hll.getCountEstimate() * 100 / samplingPercentage);
      }
    }
  }
 catch (  Exception e) {
    e.printStackTrace();
    throw e;
  }
 finally {
    IOUtils.closeStream(reader);
    tempFile.delete();
  }
  return cuboidSizeMap;
}
