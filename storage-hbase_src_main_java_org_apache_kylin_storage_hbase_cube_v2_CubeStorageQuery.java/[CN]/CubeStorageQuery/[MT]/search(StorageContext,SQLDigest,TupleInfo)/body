{
  notifyBeforeStorageQuery(sqlDigest);
  Collection<TblColRef> groups=sqlDigest.groupbyColumns;
  TupleFilter filter=sqlDigest.filter;
  Set<TblColRef> dimensions=new LinkedHashSet<TblColRef>();
  Set<FunctionDesc> metrics=new LinkedHashSet<FunctionDesc>();
  buildDimensionsAndMetrics(sqlDigest,dimensions,metrics);
  Set<TblColRef> otherDims=Sets.newHashSet(dimensions);
  otherDims.removeAll(groups);
  Set<TblColRef> derivedPostAggregation=Sets.newHashSet();
  Set<TblColRef> groupsD=expandDerived(groups,derivedPostAggregation);
  Set<TblColRef> otherDimsD=expandDerived(otherDims,derivedPostAggregation);
  otherDimsD.removeAll(groupsD);
  Set<TblColRef> dimensionsD=new LinkedHashSet<TblColRef>();
  dimensionsD.addAll(groupsD);
  dimensionsD.addAll(otherDimsD);
  Cuboid cuboid=Cuboid.identifyCuboid(cubeDesc,dimensionsD,metrics);
  context.setCuboid(cuboid);
  Set<TblColRef> singleValuesD=findSingleValueColumns(filter);
  boolean isExactAggregation=isExactAggregation(cuboid,groups,otherDimsD,singleValuesD,derivedPostAggregation);
  context.setExactAggregation(isExactAggregation);
  TupleFilter filterD=translateDerived(filter,groupsD);
  setThreshold(dimensionsD,metrics,context);
  setLimit(filter,context);
  List<CubeSegmentScanner> scanners=Lists.newArrayList();
  for (  CubeSegment cubeSeg : cubeInstance.getSegments(SegmentStatusEnum.READY)) {
    CubeSegmentScanner scanner;
    if (cubeSeg.getInputRecords() == 0) {
      logger.warn("cube segment {} input record is 0, " + "it may caused by kylin failed to the job counter " + "as the hadoop history server wasn't running",cubeSeg);
    }
    scanner=new CubeSegmentScanner(cubeSeg,cuboid,dimensionsD,groupsD,metrics,filterD,!isExactAggregation);
    scanners.add(scanner);
  }
  if (scanners.isEmpty())   return ITupleIterator.EMPTY_TUPLE_ITERATOR;
  return new SequentialCubeTupleIterator(scanners,cuboid,dimensionsD,metrics,returnTupleInfo,context);
}
