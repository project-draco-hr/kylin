{
  final CubeManager cubeManager=CubeManager.getInstance(context.getConfig());
  final CubeInstance cube=cubeManager.getCube(CubingExecutableUtil.getCubeName(this.getParams()));
  final CubeSegment segment=cube.getSegmentById(CubingExecutableUtil.getSegmentId(this.getParams()));
  Map<Integer,Long> startOffsets=KafkaOffsetMapping.parseOffsetStart(segment);
  Map<Integer,Long> endOffsets=KafkaOffsetMapping.parseOffsetEnd(segment);
  if (startOffsets.size() > 0 && endOffsets.size() > 0 && startOffsets.size() == endOffsets.size()) {
    return new ExecuteResult(ExecuteResult.State.SUCCEED,"skipped, as the offset is provided.");
  }
  final KafkaConfig kafakaConfig=KafkaConfigManager.getInstance(context.getConfig()).getKafkaConfig(cube.getFactTable());
  final String brokers=KafkaClient.getKafkaBrokers(kafakaConfig);
  final String topic=kafakaConfig.getTopic();
  try (final KafkaConsumer consumer=KafkaClient.getKafkaConsumer(brokers,cube.getName(),null)){
    final List<PartitionInfo> partitionInfos=consumer.partitionsFor(topic);
    if (startOffsets.isEmpty()) {
      for (      CubeSegment seg : cube.getSegments()) {
        Map<Integer,Long> segEndOffset=KafkaOffsetMapping.parseOffsetEnd(seg);
        for (        PartitionInfo partition : partitionInfos) {
          int partitionId=partition.partition();
          if (segEndOffset.containsKey(partitionId)) {
            startOffsets.put(partitionId,Math.max(startOffsets.containsKey(partitionId) ? startOffsets.get(partitionId) : 0,segEndOffset.get(partitionId)));
          }
        }
      }
      if (partitionInfos.size() > startOffsets.size()) {
        for (int x=startOffsets.size(); x < partitionInfos.size(); x++) {
          long earliest=KafkaClient.getEarliestOffset(consumer,topic,partitionInfos.get(x).partition());
          startOffsets.put(partitionInfos.get(x).partition(),earliest);
        }
      }
      logger.info("Get start offset for segment " + segment.getName() + ": "+ startOffsets.toString());
    }
    if (endOffsets.isEmpty()) {
      for (      PartitionInfo partitionInfo : partitionInfos) {
        long latest=KafkaClient.getLatestOffset(consumer,topic,partitionInfo.partition());
        endOffsets.put(partitionInfo.partition(),latest);
      }
      logger.info("Get end offset for segment " + segment.getName() + ": "+ endOffsets.toString());
    }
  }
   long totalStartOffset=0, totalEndOffset=0;
  for (  Long v : startOffsets.values()) {
    totalStartOffset+=v;
  }
  for (  Long v : endOffsets.values()) {
    totalEndOffset+=v;
  }
  if (totalEndOffset > totalStartOffset) {
    KafkaOffsetMapping.saveOffsetStart(segment,startOffsets);
    KafkaOffsetMapping.saveOffsetEnd(segment,endOffsets);
    segment.setName(CubeSegment.makeSegmentName(0,0,totalStartOffset,totalEndOffset));
    CubeUpdate cubeBuilder=new CubeUpdate(cube);
    cubeBuilder.setToUpdateSegs(segment);
    try {
      cubeManager.updateCube(cubeBuilder);
    }
 catch (    IOException e) {
      return new ExecuteResult(ExecuteResult.State.ERROR,e.getLocalizedMessage());
    }
    return new ExecuteResult(ExecuteResult.State.SUCCEED,"succeed, offset start: " + totalStartOffset + ", offset end: "+ totalEndOffset);
  }
 else {
    CubeUpdate cubeBuilder=new CubeUpdate(cube);
    cubeBuilder.setToRemoveSegs(segment);
    try {
      cubeManager.updateCube(cubeBuilder);
    }
 catch (    IOException e) {
      return new ExecuteResult(ExecuteResult.State.ERROR,e.getLocalizedMessage());
    }
    return new ExecuteResult(ExecuteResult.State.DISCARDED,"No new message comes");
  }
}
