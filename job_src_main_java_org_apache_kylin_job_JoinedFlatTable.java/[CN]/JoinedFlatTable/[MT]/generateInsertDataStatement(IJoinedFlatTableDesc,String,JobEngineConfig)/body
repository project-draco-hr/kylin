{
  StringBuilder sql=new StringBuilder();
  File hadoopPropertiesFile=new File(engineConfig.getHiveConfFilePath());
  if (hadoopPropertiesFile.exists()) {
    DocumentBuilderFactory factory=DocumentBuilderFactory.newInstance();
    DocumentBuilder builder;
    Document doc;
    try {
      builder=factory.newDocumentBuilder();
      doc=builder.parse(hadoopPropertiesFile);
      NodeList nl=doc.getElementsByTagName("property");
      for (int i=0; i < nl.getLength(); i++) {
        String name=doc.getElementsByTagName("name").item(i).getFirstChild().getNodeValue();
        String value=doc.getElementsByTagName("value").item(i).getFirstChild().getNodeValue();
        if (name.equals("tmpjars") == false) {
          sql.append("SET " + name + "="+ value+ ";").append("\n");
        }
      }
    }
 catch (    ParserConfigurationException e) {
      throw new IOException(e);
    }
catch (    SAXException e) {
      throw new IOException(e);
    }
  }
  sql.append("INSERT OVERWRITE TABLE " + intermediateTableDesc.getTableName(jobUUID) + " "+ generateSelectDataStatement(intermediateTableDesc)+ ";").append("\n");
  return sql.toString();
}
