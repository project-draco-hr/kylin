{
  Options options=new Options();
  try {
    options.addOption(OPTION_JOB_NAME);
    options.addOption(OPTION_CUBE_NAME);
    options.addOption(OPTION_PARTITION_FILE_PATH);
    options.addOption(OPTION_INPUT_PATH);
    options.addOption(OPTION_OUTPUT_PATH);
    options.addOption(OPTION_HTABLE_NAME);
    parseOptions(options,args);
    Path partitionFilePath=new Path(getOptionValue(OPTION_PARTITION_FILE_PATH));
    Path output=new Path(getOptionValue(OPTION_OUTPUT_PATH));
    String cubeName=getOptionValue(OPTION_CUBE_NAME).toUpperCase();
    CubeManager cubeMgr=CubeManager.getInstance(KylinConfig.getInstanceFromEnv());
    CubeInstance cube=cubeMgr.getCube(cubeName);
    job=Job.getInstance(getConf(),getOptionValue(OPTION_JOB_NAME));
    setJobClasspath(job,cube.getConfig());
    addInputDirs(getOptionValue(OPTION_INPUT_PATH),job);
    FileOutputFormat.setOutputPath(job,output);
    job.setInputFormatClass(SequenceFileInputFormat.class);
    job.setMapperClass(CubeHFileMapper.class);
    job.setReducerClass(KeyValueSortReducer.class);
    job.getConfiguration().set(BatchConstants.CFG_CUBE_NAME,cubeName);
    Configuration conf=HBaseConfiguration.create(getConf());
    attachKylinPropsAndMetadata(cube,job.getConfiguration());
    String tableName=getOptionValue(OPTION_HTABLE_NAME).toUpperCase();
    HTable htable=new HTable(conf,tableName);
    HFileOutputFormat.configureIncrementalLoad(job,htable);
    reconfigurePartitions(conf,partitionFilePath);
    conf.set(DFSConfigKeys.DFS_REPLICATION_KEY,"3");
    this.deletePath(job.getConfiguration(),output);
    return waitForCompletion(job);
  }
 catch (  Exception e) {
    logger.error("error in CubeHFileJob",e);
    printUsage(options);
    throw e;
  }
 finally {
    if (job != null)     cleanupTempConfFile(job.getConfiguration());
  }
}
