{
  HadoopUtil.setCurrentConfiguration(context.getConfiguration());
  cubeName=context.getConfiguration().get(BatchConstants.CFG_CUBE_NAME).toUpperCase();
  segmentName=context.getConfiguration().get(BatchConstants.CFG_CUBE_SEGMENT_NAME).toUpperCase();
  config=AbstractHadoopJob.loadKylinPropsAndMetadata();
  cubeManager=CubeManager.getInstance(config);
  cube=cubeManager.getCube(cubeName);
  cubeDesc=cube.getDescriptor();
  mergedCubeSegment=cube.getSegment(segmentName,SegmentStatusEnum.NEW);
  newKeyBuf=new byte[256];
  TableSplit currentSplit=(TableSplit)context.getInputSplit();
  byte[] tableName=currentSplit.getTableName();
  String htableName=Bytes.toString(tableName);
  System.out.println("htable:" + htableName);
  sourceCubeSegment=findSegmentWithHTable(htableName,cube);
  System.out.println(sourceCubeSegment);
  this.rowKeySplitter=new RowKeySplitter(sourceCubeSegment,65,255);
  List<RowValueDecoder> valueDecoderList=Lists.newArrayList();
  List<InMemKeyValueCreator> keyValueCreators=Lists.newArrayList();
  List<MeasureDesc> measuresDescs=Lists.newArrayList();
  int startPosition=0;
  for (  HBaseColumnFamilyDesc cfDesc : cubeDesc.getHBaseMapping().getColumnFamily()) {
    for (    HBaseColumnDesc colDesc : cfDesc.getColumns()) {
      valueDecoderList.add(new RowValueDecoder(colDesc));
      keyValueCreators.add(new InMemKeyValueCreator(colDesc,startPosition));
      startPosition+=colDesc.getMeasures().length;
      for (      MeasureDesc measure : colDesc.getMeasures()) {
        measuresDescs.add(measure);
      }
    }
  }
  rowValueDecoders=valueDecoderList.toArray(new RowValueDecoder[valueDecoderList.size()]);
  simpleFullCopy=(keyValueCreators.size() == 1);
  result=new Object[measuresDescs.size()];
  codec=new MeasureCodec(measuresDescs);
}
