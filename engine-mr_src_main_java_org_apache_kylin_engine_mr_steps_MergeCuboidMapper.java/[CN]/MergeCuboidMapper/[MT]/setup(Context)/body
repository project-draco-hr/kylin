{
  super.bindCurrentConfiguration(context.getConfiguration());
  cubeName=context.getConfiguration().get(BatchConstants.CFG_CUBE_NAME).toUpperCase();
  segmentName=context.getConfiguration().get(BatchConstants.CFG_CUBE_SEGMENT_NAME).toUpperCase();
  config=AbstractHadoopJob.loadKylinPropsAndMetadata();
  cubeManager=CubeManager.getInstance(config);
  cube=cubeManager.getCube(cubeName);
  cubeDesc=cube.getDescriptor();
  mergedCubeSegment=cube.getSegment(segmentName,SegmentStatusEnum.NEW);
  newKeyBodyBuf=new byte[RowConstants.ROWKEY_BUFFER_SIZE];
  newKeyBuf=ByteArray.allocate(RowConstants.ROWKEY_BUFFER_SIZE);
  FileSplit fileSplit=(FileSplit)context.getInputSplit();
  sourceCubeSegment=findSourceSegment(fileSplit,cube);
  rowKeySplitter=new RowKeySplitter(sourceCubeSegment,65,255);
  rowKeyEncoderProvider=new RowKeyEncoderProvider(mergedCubeSegment);
  measureDescs=cubeDesc.getMeasures();
  codec=new BufferedMeasureEncoder(measureDescs);
  measureObjs=new Object[measureDescs.size()];
  outputValue=new Text();
  dictMeasures=Lists.newArrayList();
  oldDicts=Maps.newHashMap();
  newDicts=Maps.newHashMap();
  for (int i=0; i < measureDescs.size(); i++) {
    MeasureDesc measureDesc=measureDescs.get(i);
    MeasureType measureType=measureDesc.getFunction().getMeasureType();
    List<TblColRef> columns=measureType.getColumnsNeedDictionary(measureDesc.getFunction());
    boolean needReEncode=false;
    for (    TblColRef col : columns) {
      if (!sourceCubeSegment.getDictionary(col).equals(mergedCubeSegment.getDictionary(col))) {
        oldDicts.put(col,sourceCubeSegment.getDictionary(col));
        newDicts.put(col,mergedCubeSegment.getDictionary(col));
        needReEncode=true;
      }
    }
    if (needReEncode) {
      dictMeasures.add(Pair.newPair(i,measureType.newIngester()));
    }
  }
}
