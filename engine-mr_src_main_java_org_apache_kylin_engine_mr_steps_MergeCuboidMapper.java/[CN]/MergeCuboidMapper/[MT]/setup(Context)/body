{
  super.bindCurrentConfiguration(context.getConfiguration());
  cubeName=context.getConfiguration().get(BatchConstants.CFG_CUBE_NAME).toUpperCase();
  segmentName=context.getConfiguration().get(BatchConstants.CFG_CUBE_SEGMENT_NAME).toUpperCase();
  config=AbstractHadoopJob.loadKylinPropsAndMetadata();
  cubeManager=CubeManager.getInstance(config);
  cube=cubeManager.getCube(cubeName);
  cubeDesc=cube.getDescriptor();
  mergedCubeSegment=cube.getSegment(segmentName,SegmentStatusEnum.NEW);
  newKeyBodyBuf=new byte[RowConstants.ROWKEY_BUFFER_SIZE];
  newKeyBuf=ByteArray.allocate(RowConstants.ROWKEY_BUFFER_SIZE);
  FileSplit fileSplit=(FileSplit)context.getInputSplit();
  sourceCubeSegment=findSourceSegment(fileSplit,cube);
  rowKeySplitter=new RowKeySplitter(sourceCubeSegment,65,255);
  rowKeyEncoderProvider=new RowKeyEncoderProvider(mergedCubeSegment);
  measureDescs=cubeDesc.getMeasures();
  codec=new MeasureCodec(measureDescs);
  measureObjs=new Object[measureDescs.size()];
  valueBuf=ByteBuffer.allocate(RowConstants.ROWVALUE_BUFFER_SIZE);
  outputValue=new Text();
  dictMeasures=Lists.newArrayList();
  for (int i=0; i < measureDescs.size(); i++) {
    MeasureDesc measureDesc=measureDescs.get(i);
    MeasureType measureType=MeasureType.create(measureDesc.getFunction());
    if (measureType.getColumnsNeedDictionary(measureDesc).isEmpty() == false) {
      dictMeasures.add(new Pair<Integer,MeasureIngester>(i,measureType.newIngester()));
    }
  }
  if (dictMeasures.size() > 0) {
    oldDicts=sourceCubeSegment.buildDictionaryMap();
    newDicts=mergedCubeSegment.buildDictionaryMap();
  }
}
