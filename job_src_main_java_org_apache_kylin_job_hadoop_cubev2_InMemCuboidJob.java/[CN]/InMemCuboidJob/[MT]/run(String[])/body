{
  Options options=new Options();
  try {
    options.addOption(OPTION_JOB_NAME);
    options.addOption(OPTION_CUBE_NAME);
    options.addOption(OPTION_SEGMENT_NAME);
    options.addOption(OPTION_INPUT_PATH);
    options.addOption(OPTION_OUTPUT_PATH);
    options.addOption(OPTION_NCUBOID_LEVEL);
    options.addOption(OPTION_INPUT_FORMAT);
    options.addOption(OPTION_TABLE_NAME);
    options.addOption(OPTION_HTABLE_NAME);
    options.addOption(OPTION_STATISTICS_OUTPUT);
    parseOptions(options,args);
    Path input=new Path(getOptionValue(OPTION_INPUT_PATH));
    Path output=new Path(getOptionValue(OPTION_OUTPUT_PATH));
    String cubeName=getOptionValue(OPTION_CUBE_NAME).toUpperCase();
    String segmentName=getOptionValue(OPTION_SEGMENT_NAME);
    String intermediateTable=getOptionValue(OPTION_TABLE_NAME);
    String htableName=getOptionValue(OPTION_HTABLE_NAME).toUpperCase();
    KylinConfig config=KylinConfig.getInstanceFromEnv();
    CubeManager cubeMgr=CubeManager.getInstance(config);
    CubeInstance cube=cubeMgr.getCube(cubeName);
    job=Job.getInstance(getConf(),getOptionValue(OPTION_JOB_NAME));
    logger.info("Starting: " + job.getJobName());
    FileInputFormat.setInputPaths(job,input);
    setJobClasspath(job);
    DataModelDesc.RealizationCapacity realizationCapacity=cube.getDescriptor().getModel().getCapacity();
    job.getConfiguration().set(BatchConstants.CUBE_CAPACITY,realizationCapacity.toString());
    String[] dbTableNames=HadoopUtil.parseHiveTableName(intermediateTable);
    HCatInputFormat.setInput(job,dbTableNames[0],dbTableNames[1]);
    job.setInputFormatClass(HCatInputFormat.class);
    job.setMapperClass(InMemCuboidMapper.class);
    job.setMapOutputKeyClass(ImmutableBytesWritable.class);
    job.setMapOutputValueClass(Text.class);
    FileOutputFormat.setOutputPath(job,output);
    job.getConfiguration().set(BatchConstants.CFG_CUBE_NAME,cubeName);
    job.getConfiguration().set(BatchConstants.CFG_CUBE_SEGMENT_NAME,segmentName);
    long timeout=1000 * 60 * 60L;
    job.getConfiguration().set("mapred.task.timeout",String.valueOf(timeout));
    Configuration conf=HBaseConfiguration.create(getConf());
    attachKylinPropsAndMetadata(cube,job.getConfiguration());
    HTable htable=new HTable(conf,htableName);
    HFileOutputFormat.configureIncrementalLoad(job,htable);
    job.setReducerClass(InMemCuboidReducer.class);
    job.setOutputKeyClass(ImmutableBytesWritable.class);
    job.setOutputValueClass(KeyValue.class);
    this.deletePath(job.getConfiguration(),output);
    return waitForCompletion(job);
  }
 catch (  Exception e) {
    logger.error("error in CuboidJob",e);
    printUsage(options);
    throw e;
  }
}
