{
  final String hiveTable=optionsHelper.getOptionValue(OPTION_INPUT_PATH);
  SparkConf conf=new SparkConf().setAppName("Simple Application");
  conf.set("spark.executor.memory","6g");
  conf.set("spark.storage.memoryFraction","0.3");
  conf.set("spark.serializer","org.apache.spark.serializer.KryoSerializer");
  conf.set("spark.kryo.registrationRequired","true");
  final Iterable<String> allClasses=Iterables.filter(Iterables.concat(Lists.newArrayList(conf.get("spark.kryo.classesToRegister","").split(",")),getKyroClasses()),new Predicate<String>(){
    @Override public boolean apply(    @Nullable String input){
      return input != null && input.trim().length() > 0;
    }
  }
);
  System.out.println("kyro classes:" + allClasses.toString());
  conf.set("spark.kryo.classesToRegister",StringUtils.join(allClasses,","));
  JavaSparkContext sc=new JavaSparkContext(conf);
  HiveContext sqlContext=new HiveContext(sc.sc());
  final DataFrame intermediateTable=sqlContext.sql("select * from " + hiveTable);
  final String cubeName=optionsHelper.getOptionValue(OPTION_CUBE_NAME);
  final String segmentId=optionsHelper.getOptionValue(OPTION_SEGMENT_ID);
  final String confPath=optionsHelper.getOptionValue(OPTION_CONF_PATH);
  final String coprocessor=optionsHelper.getOptionValue(OPTION_COPROCESSOR);
  final KylinConfig kylinConfig=KylinConfig.getInstanceFromEnv();
  kylinConfig.overrideCoprocessorLocalJar(coprocessor);
  setupClasspath(sc,confPath);
  intermediateTable.cache();
  writeDictionary(intermediateTable,cubeName,segmentId);
  final JavaRDD<List<String>> rowJavaRDD=intermediateTable.javaRDD().map(new org.apache.spark.api.java.function.Function<Row,List<String>>(){
    @Override public List<String> call(    Row v1) throws Exception {
      ArrayList<String> result=Lists.newArrayListWithExpectedSize(v1.size());
      for (int i=0; i < v1.size(); i++) {
        final Object o=v1.get(i);
        if (o != null) {
          result.add(o.toString());
        }
 else {
          result.add(null);
        }
      }
      return result;
    }
  }
);
  final Map<Long,HyperLogLogPlusCounter> samplingResult=sampling(rowJavaRDD,cubeName,segmentId);
  final byte[][] splitKeys=createHTable(cubeName,segmentId,samplingResult);
  final String hfile=build(rowJavaRDD,cubeName,segmentId,splitKeys);
  bulkLoadHFile(cubeName,segmentId,hfile);
}
