{
  super.bindCurrentConfiguration(context.getConfiguration());
  config=AbstractHadoopJob.loadKylinPropsAndMetadata();
  cubeName=context.getConfiguration().get(BatchConstants.CFG_CUBE_NAME).toUpperCase();
  segmentName=context.getConfiguration().get(BatchConstants.CFG_CUBE_SEGMENT_NAME).toUpperCase();
  cubeManager=CubeManager.getInstance(config);
  cube=cubeManager.getCube(cubeName);
  cubeDesc=cube.getDescriptor();
  mergedCubeSegment=cube.getSegment(segmentName,SegmentStatusEnum.NEW);
  storageInputFormat=MRUtil.getBatchMergeInputSide2(mergedCubeSegment).getStorageInputFormat();
  newKeyBodyBuf=new byte[RowConstants.ROWKEY_BUFFER_SIZE];
  newKeyBuf=ByteArray.allocate(RowConstants.ROWKEY_BUFFER_SIZE);
  sourceCubeSegment=storageInputFormat.findSourceSegment(context);
  logger.info("Source cube segment: " + sourceCubeSegment);
  rowKeySplitter=new RowKeySplitter(sourceCubeSegment,65,255);
  rowKeyEncoderProvider=new RowKeyEncoderProvider(mergedCubeSegment);
  measuresDescs=cubeDesc.getMeasures();
  codec=new MeasureCodec(cubeDesc.getMeasures());
  if (cubeDesc.hasMeasureUsingDictionary()) {
    List<Integer> measuresUsingDict=Lists.newArrayList();
    for (int i=0; i < measuresDescs.size(); i++) {
      if (measuresDescs.get(i).getFunction().isTopN()) {
        measuresUsingDict.add(i);
      }
    }
    measureIdxUsingDict=measuresUsingDict.toArray(new Integer[measuresUsingDict.size()]);
  }
}
