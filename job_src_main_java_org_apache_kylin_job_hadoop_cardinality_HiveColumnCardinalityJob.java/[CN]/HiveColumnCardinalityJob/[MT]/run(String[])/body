{
  Options options=new Options();
  try {
    options.addOption(OPTION_TABLE);
    options.addOption(OPTION_OUTPUT_PATH);
    parseOptions(options,args);
    String jobName=JOB_TITLE + getOptionsAsString();
    System.out.println("Starting: " + jobName);
    Configuration conf=getConf();
    job=Job.getInstance(conf,jobName);
    setJobClasspath(job);
    String table=getOptionValue(OPTION_TABLE);
    job.getConfiguration().set(BatchConstants.TABLE_NAME,table);
    Path output=new Path(getOptionValue(OPTION_OUTPUT_PATH));
    FileOutputFormat.setOutputPath(job,output);
    job.getConfiguration().set("dfs.block.size","67108864");
    IMRTableInputFormat tableInputFormat=MRBatchCubingEngine.getTableInputFormat(table);
    tableInputFormat.configureJob(job);
    job.setMapperClass(ColumnCardinalityMapper.class);
    job.setMapOutputKeyClass(IntWritable.class);
    job.setMapOutputValueClass(BytesWritable.class);
    job.setReducerClass(ColumnCardinalityReducer.class);
    job.setOutputFormatClass(TextOutputFormat.class);
    job.setOutputKeyClass(IntWritable.class);
    job.setOutputValueClass(LongWritable.class);
    job.setNumReduceTasks(1);
    this.deletePath(job.getConfiguration(),output);
    System.out.println("Going to submit HiveColumnCardinalityJob for table '" + table + "'");
    int result=waitForCompletion(job);
    return result;
  }
 catch (  Exception e) {
    printUsage(options);
    throw e;
  }
}
