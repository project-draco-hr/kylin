{
  final String mapReduceJobClass=getMapReduceJobClass();
  String params=getMapReduceParams();
  Preconditions.checkNotNull(mapReduceJobClass);
  Preconditions.checkNotNull(params);
  try {
    final Constructor<? extends AbstractHadoopJob> constructor=(Constructor<? extends AbstractHadoopJob>)Class.forName(mapReduceJobClass).getConstructor();
    final AbstractHadoopJob job=constructor.newInstance();
    job.setAsync(true);
    String[] args=params.trim().split("\\s+");
    ToolRunner.run(job,args);
    final HadoopCmdOutput hadoopCmdOutput=new HadoopCmdOutput(context.getConfig().getYarnStatusServiceUrl(),job);
    JobStepStatusEnum status;
    do {
      status=hadoopCmdOutput.getStatus();
      jobService.updateJobInfo(this,job.getInfo());
      if (status.isComplete()) {
        break;
      }
      Thread.sleep(context.getConfig().getYarnStatusCheckIntervalSeconds() * 1000);
    }
 while (!stopped);
    if (status.isComplete()) {
      final Map<String,String> info=job.getInfo();
      info.put(JobInstance.SOURCE_RECORDS_COUNT,hadoopCmdOutput.getMapInputRecords());
      info.put(JobInstance.HDFS_BYTES_WRITTEN,hadoopCmdOutput.getHdfsBytesWritten());
      jobService.updateJobInfo(this,info);
      if (status == JobStepStatusEnum.FINISHED) {
        return new ExecuteResult(ExecuteResult.State.SUCCEED,hadoopCmdOutput.getOutput());
      }
 else {
        return new ExecuteResult(ExecuteResult.State.FAILED,hadoopCmdOutput.getOutput());
      }
    }
 else {
      return new ExecuteResult(ExecuteResult.State.STOPPED,hadoopCmdOutput.getOutput());
    }
  }
 catch (  ReflectiveOperationException e) {
    logger.error("error getMapReduceJobClass, class name:" + getParam(KEY_MR_JOB),e);
    throw new ExecuteException(e);
  }
catch (  Exception e) {
    logger.error("error execute MapReduceJob, id:" + getId(),e);
    return new ExecuteResult(ExecuteResult.State.ERROR,e.getLocalizedMessage());
  }
}
