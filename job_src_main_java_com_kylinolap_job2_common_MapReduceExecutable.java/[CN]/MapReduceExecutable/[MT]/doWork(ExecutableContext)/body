{
  final String mapReduceJobClass=getMapReduceJobClass();
  String params=getMapReduceParams();
  Preconditions.checkNotNull(mapReduceJobClass);
  Preconditions.checkNotNull(params);
  try {
    final Constructor<? extends AbstractHadoopJob> constructor=(Constructor<? extends AbstractHadoopJob>)Class.forName(mapReduceJobClass).getConstructor();
    final AbstractHadoopJob job=constructor.newInstance();
    job.setAsync(true);
    String[] args=params.trim().split("\\s+");
    ToolRunner.run(job,args);
    final HadoopCmdOutput hadoopCmdOutput=new HadoopCmdOutput(context.getConfig().getYarnStatusServiceUrl(),job);
    JobStepStatusEnum status;
    do {
      status=hadoopCmdOutput.getStatus();
      jobService.updateJobInfo(getId(),job.getInfo());
      if (status.isComplete()) {
        final Map<String,String> info=job.getInfo();
        info.put(JobInstance.SOURCE_RECORDS_COUNT,hadoopCmdOutput.getMapInputRecords());
        info.put(JobInstance.HDFS_BYTES_WRITTEN,hadoopCmdOutput.getHdfsBytesWritten());
        jobService.updateJobInfo(getId(),info);
        if (status == JobStepStatusEnum.FINISHED) {
          return new ExecuteResult(ExecuteResult.State.SUCCEED,hadoopCmdOutput.getOutput());
        }
 else {
          return new ExecuteResult(ExecuteResult.State.FAILED,hadoopCmdOutput.getOutput());
        }
      }
      Thread.sleep(context.getConfig().getYarnStatusCheckIntervalSeconds() * 1000);
    }
 while (!isStopped());
    return new ExecuteResult(ExecuteResult.State.STOPPED,hadoopCmdOutput.getOutput());
  }
 catch (  ReflectiveOperationException e) {
    logger.error("error getMapReduceJobClass, class name:" + getParam(KEY_MR_JOB),e);
    return new ExecuteResult(ExecuteResult.State.ERROR,e.getLocalizedMessage());
  }
catch (  Exception e) {
    logger.error("error execute MapReduceJob, id:" + getId(),e);
    return new ExecuteResult(ExecuteResult.State.ERROR,e.getLocalizedMessage());
  }
}
