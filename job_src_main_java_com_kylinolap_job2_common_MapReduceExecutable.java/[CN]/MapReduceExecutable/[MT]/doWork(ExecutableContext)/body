{
  final String mapReduceJobClass=getMapReduceJobClass();
  String params=getMapReduceParams();
  Preconditions.checkNotNull(mapReduceJobClass);
  Preconditions.checkNotNull(params);
  try {
    Job job;
    final Map<String,String> extra=jobService.getOutput(getId()).getExtra();
    if (extra.containsKey(ExecutableConstants.MR_JOB_ID)) {
      job=new Cluster(new Configuration()).getJob(JobID.forName(extra.get(ExecutableConstants.MR_JOB_ID)));
      logger.info("mr_job_id:" + extra.get(ExecutableConstants.MR_JOB_ID + " resumed"));
    }
 else {
      final Constructor<? extends AbstractHadoopJob> constructor=(Constructor<? extends AbstractHadoopJob>)Class.forName(mapReduceJobClass).getConstructor();
      final AbstractHadoopJob hadoopJob=constructor.newInstance();
      hadoopJob.setAsync(true);
      String[] args=params.trim().split("\\s+");
      ToolRunner.run(hadoopJob,args);
      job=hadoopJob.getJob();
    }
    final StringBuilder output=new StringBuilder();
    final HadoopCmdOutput hadoopCmdOutput=new HadoopCmdOutput(job,output);
    String mrJobId=hadoopCmdOutput.getMrJobId();
    HadoopStatusChecker statusChecker=new HadoopStatusChecker(context.getConfig().getYarnStatusServiceUrl(),mrJobId,output);
    JobStepStatusEnum status=JobStepStatusEnum.NEW;
    while (!isDiscarded()) {
      JobStepStatusEnum newStatus=statusChecker.checkStatus();
      if (status == JobStepStatusEnum.WAITING && (newStatus == JobStepStatusEnum.FINISHED || newStatus == JobStepStatusEnum.ERROR || newStatus == JobStepStatusEnum.RUNNING)) {
        final long waitTime=System.currentTimeMillis() - getStartTime();
        addExtraInfo(MAP_REDUCE_WAIT_TIME,Long.toString(waitTime));
      }
      status=newStatus;
      jobService.addJobInfo(getId(),hadoopCmdOutput.getInfo());
      if (status.isComplete()) {
        hadoopCmdOutput.updateJobCounter();
        final Map<String,String> info=hadoopCmdOutput.getInfo();
        info.put(ExecutableConstants.SOURCE_RECORDS_COUNT,hadoopCmdOutput.getMapInputRecords());
        info.put(ExecutableConstants.HDFS_BYTES_WRITTEN,hadoopCmdOutput.getHdfsBytesWritten());
        jobService.addJobInfo(getId(),info);
        if (status == JobStepStatusEnum.FINISHED) {
          return new ExecuteResult(ExecuteResult.State.SUCCEED,output.toString());
        }
 else {
          return new ExecuteResult(ExecuteResult.State.FAILED,output.toString());
        }
      }
      Thread.sleep(context.getConfig().getYarnStatusCheckIntervalSeconds() * 1000);
    }
    return new ExecuteResult(ExecuteResult.State.DISCARDED,output.toString());
  }
 catch (  ReflectiveOperationException e) {
    logger.error("error getMapReduceJobClass, class name:" + getParam(KEY_MR_JOB),e);
    return new ExecuteResult(ExecuteResult.State.ERROR,e.getLocalizedMessage());
  }
catch (  Exception e) {
    logger.error("error execute MapReduceJob, id:" + getId(),e);
    return new ExecuteResult(ExecuteResult.State.ERROR,e.getLocalizedMessage());
  }
}
