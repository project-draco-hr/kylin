{
  List<KafkaClusterConfig> allClustersConfigs=streamingConfig.getKafkaClusterConfigs();
  int clusterID=0;
  if (allClustersConfigs.size() != 1) {
    throw new RuntimeException("II streaming only support one kafka cluster");
  }
  KafkaClusterConfig kafkaClusterConfig=allClustersConfigs.get(0);
  final int partitionCount=KafkaRequester.getKafkaTopicMeta(kafkaClusterConfig).getPartitionIds().size();
  Preconditions.checkArgument(partitionId >= 0 && partitionId < partitionCount,"invalid partition id:" + partitionId);
  final IIInstance ii=IIManager.getInstance(this.kylinConfig).getII(streamingConfig.getIiName());
  Preconditions.checkNotNull(ii,"cannot find ii name:" + streamingConfig.getIiName());
  Preconditions.checkArgument(ii.getSegments().size() > 0);
  final IISegment iiSegment=ii.getSegments().get(0);
  final Broker leadBroker=StreamingUtil.getLeadBroker(kafkaClusterConfig,partitionId);
  Preconditions.checkState(leadBroker != null,"cannot find lead broker");
  final int shard=ii.getDescriptor().getSharding();
  Preconditions.checkArgument(shard % partitionCount == 0);
  final int parallelism=shard / partitionCount;
  final int startShard=partitionId * parallelism;
  final int endShard=startShard + parallelism;
  long streamingOffset=getEarliestStreamingOffset(streamingConfig.getName(),startShard,endShard);
  streamingOffset=streamingOffset - (streamingOffset % parallelism);
  logger.info("offset from ii desc is " + streamingOffset);
  final long earliestOffset=KafkaRequester.getLastOffset(kafkaClusterConfig.getTopic(),partitionId,OffsetRequest.EarliestTime(),leadBroker,kafkaClusterConfig);
  logger.info("offset from KafkaRequester is " + earliestOffset);
  streamingOffset=Math.max(streamingOffset,earliestOffset);
  logger.info("starting offset is " + streamingOffset);
  if (!HBaseConnection.tableExists(kylinConfig.getStorageUrl(),iiSegment.getStorageLocationIdentifier())) {
    logger.error("no htable:" + iiSegment.getStorageLocationIdentifier() + " found");
    throw new IllegalStateException("please create htable:" + iiSegment.getStorageLocationIdentifier() + " first");
  }
  KafkaConsumer consumer=new KafkaConsumer(clusterID,kafkaClusterConfig.getTopic(),partitionId,streamingOffset,kafkaClusterConfig.getBrokers(),kafkaClusterConfig,parallelism);
  kafkaConsumers.put(getKey(streamingConfig.getName(),partitionId),consumer);
  final IIDesc iiDesc=iiSegment.getIIDesc();
  Executors.newSingleThreadExecutor().submit(consumer);
  final ExecutorService streamingBuilderPool=Executors.newFixedThreadPool(parallelism);
  for (int i=startShard; i < endShard; ++i) {
    final StreamBuilder task=StreamBuilder.newLimitedSizeStreamBuilder(streamingConfig.getName(),consumer.getStreamQueue(i % parallelism),new IIStreamConsumer(streamingConfig.getName(),iiSegment.getStorageLocationIdentifier(),iiDesc,i),0L,iiDesc.getSliceSize());
    task.setStreamParser(getStreamParser(streamingConfig,ii.getDescriptor().listAllColumns()));
    if (i == endShard - 1) {
      streamingBuilderPool.submit(task).get();
    }
 else {
      streamingBuilderPool.submit(task);
    }
  }
}
