{
  final KafkaConfig kafkaConfig=streamManager.getKafkaConfig(streaming);
  Preconditions.checkArgument(kafkaConfig != null,"cannot find kafka config:" + streaming);
  final IIInstance ii=iiManager.getII(kafkaConfig.getIiName());
  Preconditions.checkNotNull(ii,"cannot find ii name:" + kafkaConfig.getIiName());
  Preconditions.checkArgument(partitionId >= 0 && partitionId < ii.getDescriptor().getSharding(),"invalid partition id:" + partitionId);
  Preconditions.checkArgument(ii.getSegments().size() > 0);
  final IISegment iiSegment=ii.getSegments().get(0);
  final ByteArrayOutputStream out=new ByteArrayOutputStream();
  KafkaConfig.SERIALIZER.serialize(kafkaConfig,new DataOutputStream(out));
  logger.debug("kafka config:" + new String(out.toByteArray()));
  final Broker leadBroker=getLeadBroker(kafkaConfig,partitionId);
  Preconditions.checkState(leadBroker != null,"cannot find lead broker");
  final long earliestOffset=KafkaRequester.getLastOffset(kafkaConfig.getTopic(),partitionId,OffsetRequest.EarliestTime(),leadBroker,kafkaConfig);
  long streamOffset=streamManager.getOffset(streaming,partitionId);
  if (streamOffset < earliestOffset) {
    streamOffset=earliestOffset;
  }
  if (!HBaseConnection.tableExists(kylinConfig.getStorageUrl(),iiSegment.getStorageLocationIdentifier())) {
    logger.error("no htable:" + iiSegment.getStorageLocationIdentifier() + " found");
    throw new IllegalStateException("please create htable:" + iiSegment.getStorageLocationIdentifier() + " first");
  }
  KafkaConsumer consumer=new KafkaConsumer(kafkaConfig.getTopic(),partitionId,streamOffset,kafkaConfig.getBrokers(),kafkaConfig){
    @Override protected void consume(    long offset,    ByteBuffer payload) throws Exception {
      byte[] bytes=new byte[payload.limit()];
      payload.get(bytes);
      getStreamQueue().put(new Stream(offset,bytes));
    }
  }
;
  kafkaConsumers.put(getKey(streaming,partitionId),consumer);
  final IIStreamBuilder task=new IIStreamBuilder(consumer.getStreamQueue(),streaming,iiSegment.getStorageLocationIdentifier(),iiSegment.getIIDesc(),partitionId);
  StreamParser parser;
  if (!StringUtils.isEmpty(kafkaConfig.getParserName())) {
    Class clazz=Class.forName(kafkaConfig.getParserName());
    Constructor constructor=clazz.getConstructor(List.class);
    parser=(StreamParser)constructor.newInstance(ii.getDescriptor().listAllColumns());
  }
 else {
    parser=new JsonStreamParser(ii.getDescriptor().listAllColumns());
  }
  task.setStreamParser(parser);
  Executors.newSingleThreadExecutor().submit(consumer);
  Executors.newSingleThreadExecutor().submit(task).get();
}
