{
  List<KafkaClusterConfig> kafkaClusterConfigs=streamingConfig.getKafkaClusterConfigs();
  final List<BlockingQueue<StreamMessage>> allClustersData=Lists.newArrayList();
  for (  KafkaClusterConfig kafkaClusterConfig : kafkaClusterConfigs) {
    final int partitionCount=KafkaRequester.getKafkaTopicMeta(kafkaClusterConfig).getPartitionIds().size();
    Preconditions.checkArgument(partitionId >= 0 && partitionId < partitionCount,"invalid partition id:" + partitionId);
    final List<BlockingQueue<StreamMessage>> oneClusterData=consume(kafkaClusterConfig,partitionCount);
    logger.info("Cluster {} with {} partitions",allClustersData.size(),oneClusterData.size());
    allClustersData.addAll(oneClusterData);
  }
  final String cubeName=streamingConfig.getCubeName();
  final CubeInstance cubeInstance=CubeManager.getInstance(kylinConfig).getCube(cubeName);
  int batchInterval=5 * 60 * 1000;
  MicroBatchCondition condition=new MicroBatchCondition(Integer.MAX_VALUE,batchInterval);
  long startTimestamp=cubeInstance.getDateRangeEnd() == 0 ? TimeUtil.getNextPeriodStart(System.currentTimeMillis(),(long)batchInterval) : cubeInstance.getDateRangeEnd();
  StreamBuilder cubeStreamBuilder=new StreamBuilder(allClustersData,condition,new CubeStreamConsumer(cubeName),startTimestamp);
  cubeStreamBuilder.setStreamParser(getStreamParser(streamingConfig,cubeInstance.getAllColumns()));
  cubeStreamBuilder.setStreamFilter(getStreamFilter(streamingConfig));
  final Future<?> future=Executors.newSingleThreadExecutor().submit(cubeStreamBuilder);
  future.get();
}
