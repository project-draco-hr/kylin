{
  List<KafkaClusterConfig> kafkaClusterConfigs=streamingConfig.getKafkaClusterConfigs();
  final List<List<BlockingQueue<StreamMessage>>> allClustersData=Lists.newArrayList();
  for (  KafkaClusterConfig kafkaClusterConfig : kafkaClusterConfigs) {
    final int partitionCount=KafkaRequester.getKafkaTopicMeta(kafkaClusterConfig).getPartitionIds().size();
    Preconditions.checkArgument(partitionId >= 0 && partitionId < partitionCount,"invalid partition id:" + partitionId);
    final List<BlockingQueue<StreamMessage>> oneClusterData=consume(kafkaClusterConfig,partitionCount);
    logger.info("Cluster {} with {} partitions",allClustersData.size(),oneClusterData.size());
    allClustersData.add(oneClusterData);
  }
  final LinkedBlockingDeque<StreamMessage> alldata=new LinkedBlockingDeque<>();
  Executors.newSingleThreadExecutor().execute(new Runnable(){
    @Override public void run(){
      int totalMessage=0;
      while (true) {
        for (        List<BlockingQueue<StreamMessage>> oneCluster : allClustersData) {
          for (          BlockingQueue<StreamMessage> onePartition : oneCluster) {
            try {
              alldata.put(onePartition.take());
              if (totalMessage++ % 10000 == 0) {
                logger.info("Total stream message count: " + totalMessage);
              }
            }
 catch (            InterruptedException e) {
              throw new RuntimeException(e);
            }
          }
        }
      }
    }
  }
);
  final String cubeName=streamingConfig.getCubeName();
  final CubeInstance cubeInstance=CubeManager.getInstance(kylinConfig).getCube(cubeName);
  CubeStreamBuilder cubeStreamBuilder=new CubeStreamBuilder(alldata,cubeName);
  cubeStreamBuilder.setStreamParser(getStreamParser(streamingConfig,cubeInstance.getAllColumns()));
  cubeStreamBuilder.setStreamFilter(getStreamFilter(streamingConfig));
  final Future<?> future=Executors.newSingleThreadExecutor().submit(cubeStreamBuilder);
  future.get();
}
