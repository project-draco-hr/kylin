{
  List<KafkaClusterConfig> kafkaClusterConfigs=streamingConfig.getKafkaClusterConfigs();
  final List<BlockingQueue<StreamMessage>> allClustersData=Lists.newArrayList();
  ArrayList<Integer> allPartitions=Lists.newArrayList();
  int partitionOffset=0;
  for (  KafkaClusterConfig kafkaClusterConfig : kafkaClusterConfigs) {
    final int partitionCount=KafkaRequester.getKafkaTopicMeta(kafkaClusterConfig).getPartitionIds().size();
    for (int i=0; i < partitionCount; i++) {
      allPartitions.add(i + partitionOffset);
    }
    partitionOffset+=partitionCount;
  }
  final Map<Integer,Long> partitionIdOffsetMap=streamingManager.getOffset(streamingConfig.getName(),allPartitions);
  int clusterID=0;
  partitionOffset=0;
  for (  KafkaClusterConfig kafkaClusterConfig : kafkaClusterConfigs) {
    final int partitionCount=KafkaRequester.getKafkaTopicMeta(kafkaClusterConfig).getPartitionIds().size();
    Preconditions.checkArgument(partitionId >= 0 && partitionId < partitionCount,"invalid partition id:" + partitionId);
    final List<BlockingQueue<StreamMessage>> oneClusterData=consume(clusterID,kafkaClusterConfig,partitionCount,partitionIdOffsetMap,partitionOffset);
    logger.info("Cluster {} with {} partitions",allClustersData.size(),oneClusterData.size());
    allClustersData.addAll(oneClusterData);
    clusterID++;
    partitionOffset+=partitionCount;
  }
  final String cubeName=streamingConfig.getCubeName();
  final CubeInstance cubeInstance=CubeManager.getInstance(kylinConfig).getCube(cubeName);
  int batchInterval=5 * 60 * 1000;
  MicroBatchCondition condition=new MicroBatchCondition(Integer.MAX_VALUE,batchInterval);
  long startTimestamp=cubeInstance.getDateRangeEnd() == 0 ? TimeUtil.getNextPeriodStart(System.currentTimeMillis(),(long)batchInterval) : cubeInstance.getDateRangeEnd();
  logger.info("batch time interval is {} to {}",DateFormat.formatToTimeStr(startTimestamp),DateFormat.formatToTimeStr(startTimestamp + batchInterval));
  StreamBuilder cubeStreamBuilder=new StreamBuilder(streamingConfig.getName(),allClustersData,condition,new CubeStreamConsumer(cubeName),startTimestamp);
  cubeStreamBuilder.setStreamParser(getStreamParser(streamingConfig,Lists.transform(new CubeJoinedFlatTableDesc(cubeInstance.getDescriptor(),null).getColumnList(),new Function<IntermediateColumnDesc,TblColRef>(){
    @Nullable @Override public TblColRef apply(    IntermediateColumnDesc input){
      return input.getColRef();
    }
  }
)));
  final Future<?> future=Executors.newSingleThreadExecutor().submit(cubeStreamBuilder);
  future.get();
}
