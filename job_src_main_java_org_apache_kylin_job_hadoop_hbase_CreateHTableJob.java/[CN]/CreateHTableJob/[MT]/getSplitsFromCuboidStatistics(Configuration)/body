{
  List<Integer> rowkeyColumnSize=Lists.newArrayList();
  CubeSegment cubeSegment=cube.getSegment(segmentName,SegmentStatusEnum.NEW);
  long baseCuboidId=Cuboid.getBaseCuboidId(cubeDesc);
  Cuboid baseCuboid=Cuboid.findById(cubeDesc,baseCuboidId);
  List<TblColRef> columnList=baseCuboid.getColumns();
  for (int i=0; i < columnList.size(); i++) {
    logger.info("Rowkey column " + i + " length "+ cubeSegment.getColumnLength(columnList.get(i)));
    rowkeyColumnSize.add(cubeSegment.getColumnLength(columnList.get(i)));
  }
  DataModelDesc.RealizationCapacity cubeCapacity=cubeDesc.getModel().getCapacity();
  int cut=kylinConfig.getHBaseRegionCut(cubeCapacity.toString());
  logger.info("Cube capacity " + cubeCapacity.toString() + ", chosen cut for HTable is "+ cut+ "GB");
  Map<Long,Long> cuboidSizeMap=Maps.newHashMap();
  long totalSizeInM=0;
  ResourceStore rs=ResourceStore.getStore(kylinConfig);
  String fileKey=ResourceStore.CUBE_STATISTICS_ROOT + "/" + cube.getName()+ "/"+ cubeSegment.getUuid()+ ".seq";
  InputStream is=rs.getResource(fileKey);
  File tempFile=null;
  FileOutputStream tempFileStream=null;
  try {
    tempFile=File.createTempFile(cubeSegment.getUuid(),".seq");
    tempFileStream=new FileOutputStream(tempFile);
    org.apache.commons.io.IOUtils.copy(is,tempFileStream);
  }
  finally {
    IOUtils.closeStream(is);
    IOUtils.closeStream(tempFileStream);
  }
  FileSystem fs=HadoopUtil.getFileSystem("file:///" + tempFile.getAbsolutePath());
  SequenceFile.Reader reader=null;
  try {
    reader=new SequenceFile.Reader(fs,new Path(tempFile.getAbsolutePath()),conf);
    LongWritable key=(LongWritable)ReflectionUtils.newInstance(reader.getKeyClass(),conf);
    BytesWritable value=(BytesWritable)ReflectionUtils.newInstance(reader.getValueClass(),conf);
    while (reader.next(key,value)) {
      HyperLogLogPlusCounter hll=new HyperLogLogPlusCounter(16);
      ByteArray byteArray=new ByteArray(value.getBytes());
      hll.readRegisters(byteArray.asBuffer());
      cuboidSizeMap.put(key.get(),hll.getCountEstimate() * 100 / SAMPING_PERCENTAGE);
    }
  }
 catch (  Exception e) {
    e.printStackTrace();
    throw e;
  }
 finally {
    IOUtils.closeStream(reader);
  }
  List<Long> allCuboids=Lists.newArrayList();
  allCuboids.addAll(cuboidSizeMap.keySet());
  Collections.sort(allCuboids);
  for (  long cuboidId : allCuboids) {
    long cuboidSize=estimateCuboidStorageSize(cuboidId,cuboidSizeMap.get(cuboidId),baseCuboidId,rowkeyColumnSize);
    cuboidSizeMap.put(cuboidId,cuboidSize);
    totalSizeInM+=cuboidSize;
  }
  int nRegion=Math.round((float)totalSizeInM / ((float)cut * 1024l));
  nRegion=Math.max(1,nRegion);
  nRegion=Math.min(MAX_REGION,nRegion);
  int mbPerRegion=(int)(totalSizeInM / (nRegion));
  mbPerRegion=Math.max(1,mbPerRegion);
  logger.info("Total size " + totalSizeInM + "M (estimated)");
  logger.info(nRegion + " regions (estimated)");
  logger.info(mbPerRegion + " MB per region (estimated)");
  List<Long> regionSplit=Lists.newArrayList();
  long size=0;
  int regionIndex=0;
  int cuboidCount=0;
  for (int i=0; i < allCuboids.size(); i++) {
    long cuboidId=allCuboids.get(i);
    if (size >= mbPerRegion || (size + cuboidSizeMap.get(cuboidId)) >= mbPerRegion * 1.2) {
      regionSplit.add(cuboidId);
      logger.info("Region " + regionIndex + " will be "+ size+ " MB, contains cuboids < "+ cuboidId+ " ("+ cuboidCount+ ") cuboids");
      size=0;
      cuboidCount=0;
      regionIndex++;
    }
    size+=cuboidSizeMap.get(cuboidId);
    cuboidCount++;
  }
  byte[][] result=new byte[regionSplit.size()][];
  for (int i=0; i < regionSplit.size(); i++) {
    result[i]=Bytes.toBytes(regionSplit.get(i));
  }
  return result;
}
