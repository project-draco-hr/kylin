{
  super.bindCurrentConfiguration(context.getConfiguration());
  cubeName=context.getConfiguration().get(BatchConstants.CFG_CUBE_NAME).toUpperCase();
  segmentID=context.getConfiguration().get(BatchConstants.CFG_CUBE_SEGMENT_ID);
  intermediateTableRowDelimiter=context.getConfiguration().get(BatchConstants.CFG_CUBE_INTERMEDIATE_TABLE_ROW_DELIMITER,Character.toString(BatchConstants.INTERMEDIATE_TABLE_ROW_DELIMITER));
  if (Bytes.toBytes(intermediateTableRowDelimiter).length > 1) {
    throw new RuntimeException("Expected delimiter byte length is 1, but got " + Bytes.toBytes(intermediateTableRowDelimiter).length);
  }
  byteRowDelimiter=Bytes.toBytes(intermediateTableRowDelimiter)[0];
  KylinConfig config=AbstractHadoopJob.loadKylinPropsAndMetadata();
  cube=CubeManager.getInstance(config).getCube(cubeName);
  cubeDesc=cube.getDescriptor();
  cubeSegment=cube.getSegmentById(segmentID);
  long baseCuboidId=Cuboid.getBaseCuboidId(cubeDesc);
  baseCuboid=Cuboid.findById(cubeDesc,baseCuboidId);
  intermediateTableDesc=new CubeJoinedFlatTableEnrich(EngineFactory.getJoinedFlatTableDesc(cubeSegment),cubeDesc);
  bytesSplitter=new BytesSplitter(200,16384);
  rowKeyEncoder=AbstractRowKeyEncoder.createInstance(cubeSegment,baseCuboid);
  measureCodec=new BufferedMeasureCodec(cubeDesc.getMeasures());
  measures=new Object[cubeDesc.getMeasures().size()];
  int colCount=cubeDesc.getRowkey().getRowKeyColumns().length;
  keyBytesBuf=new byte[colCount][];
  aggrIngesters=MeasureIngester.create(cubeDesc.getMeasures());
  dictionaryMap=cubeSegment.buildDictionaryMap();
  initNullBytes();
}
