{
  Options options=new Options();
  try {
    options.addOption(OPTION_JOB_NAME);
    options.addOption(OPTION_CUBE_NAME);
    options.addOption(OPTION_SEGMENT_ID);
    options.addOption(OPTION_OUTPUT_PATH);
    options.addOption(OPTION_CUBING_JOB_ID);
    parseOptions(options,args);
    String cubeName=getOptionValue(OPTION_CUBE_NAME).toUpperCase();
    String segmentID=getOptionValue(OPTION_SEGMENT_ID);
    String output=getOptionValue(OPTION_OUTPUT_PATH);
    CubeManager cubeMgr=CubeManager.getInstance(KylinConfig.getInstanceFromEnv());
    CubeInstance cube=cubeMgr.getCube(cubeName);
    CubeSegment cubeSeg=cube.getSegmentById(segmentID);
    String cubingJobId=getOptionValue(OPTION_CUBING_JOB_ID);
    if (checkSkip(cubingJobId)) {
      logger.info("Skip job " + getOptionValue(OPTION_JOB_NAME) + " for "+ cubeSeg);
      return 0;
    }
    job=Job.getInstance(getConf(),getOptionValue(OPTION_JOB_NAME));
    job.getConfiguration().set(BatchConstants.ARG_CUBING_JOB_ID,cubingJobId);
    logger.info("Starting: " + job.getJobName());
    setJobClasspath(job,cube.getConfig());
    attachKylinPropsAndMetadata(cube,job.getConfiguration());
    job.getConfiguration().set(BatchConstants.CFG_CUBE_NAME,cubeName);
    job.getConfiguration().set(BatchConstants.CFG_CUBE_SEGMENT_ID,segmentID);
    IMRTableInputFormat flatTableInputFormat=MRUtil.getBatchCubingInputSide(cubeSeg).getFlatTableInputFormat();
    flatTableInputFormat.configureJob(job);
    job.setMapperClass(InMemCuboidMapper.class);
    job.setMapOutputKeyClass(ByteArrayWritable.class);
    job.setMapOutputValueClass(ByteArrayWritable.class);
    job.setReducerClass(InMemCuboidReducer.class);
    job.setNumReduceTasks(calculateReducerNum(cubeSeg));
    job.setOutputFormatClass(SequenceFileOutputFormat.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(Text.class);
    Path outputPath=new Path(output);
    FileOutputFormat.setOutputPath(job,outputPath);
    HadoopUtil.deletePath(job.getConfiguration(),outputPath);
    return waitForCompletion(job);
  }
 catch (  Exception e) {
    logger.error("error in CuboidJob",e);
    printUsage(options);
    throw e;
  }
 finally {
    if (job != null)     cleanupTempConfFile(job.getConfiguration());
  }
}
