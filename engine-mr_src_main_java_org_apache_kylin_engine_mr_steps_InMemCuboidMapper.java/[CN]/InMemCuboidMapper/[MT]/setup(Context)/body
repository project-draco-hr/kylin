{
  super.bindCurrentConfiguration(context.getConfiguration());
  Configuration conf=context.getConfiguration();
  KylinConfig config=AbstractHadoopJob.loadKylinPropsAndMetadata();
  String cubeName=conf.get(BatchConstants.CFG_CUBE_NAME);
  cube=CubeManager.getInstance(config).getCube(cubeName);
  cubeDesc=cube.getDescriptor();
  String segmentName=context.getConfiguration().get(BatchConstants.CFG_CUBE_SEGMENT_NAME);
  cubeSegment=cube.getSegment(segmentName,SegmentStatusEnum.NEW);
  flatTableInputFormat=MRUtil.getBatchCubingInputSide(cubeSegment).getFlatTableInputFormat();
  Map<TblColRef,Dictionary<String>> dictionaryMap=Maps.newHashMap();
  for (  TblColRef col : cubeDesc.getAllColumnsNeedDictionary()) {
    Dictionary<?> dict=cubeSegment.getDictionary(col);
    if (dict == null) {
      logger.warn("Dictionary for " + col + " was not found.");
    }
    dictionaryMap.put(col,cubeSegment.getDictionary(col));
  }
  DoggedCubeBuilder cubeBuilder=new DoggedCubeBuilder(cube.getDescriptor(),dictionaryMap);
  ExecutorService executorService=Executors.newSingleThreadExecutor();
  future=executorService.submit(cubeBuilder.buildAsRunnable(queue,new MapContextGTRecordWriter(context,cubeDesc,cubeSegment)));
}
