{
  super.bindCurrentConfiguration(context.getConfiguration());
  Configuration conf=context.getConfiguration();
  KylinConfig config=AbstractHadoopJob.loadKylinPropsAndMetadata();
  String cubeName=conf.get(BatchConstants.CFG_CUBE_NAME);
  cube=CubeManager.getInstance(config).getCube(cubeName);
  cubeDesc=cube.getDescriptor();
  String segmentID=context.getConfiguration().get(BatchConstants.CFG_CUBE_SEGMENT_ID);
  cubeSegment=cube.getSegmentById(segmentID);
  flatTableInputFormat=MRUtil.getBatchCubingInputSide(cubeSegment).getFlatTableInputFormat();
  IJoinedFlatTableDesc flatDesc=EngineFactory.getJoinedFlatTableDesc(cubeSegment);
  Map<TblColRef,Dictionary<String>> dictionaryMap=Maps.newHashMap();
  for (  TblColRef col : cubeDesc.getAllColumnsHaveDictionary()) {
    Dictionary<?> dict=cubeSegment.getDictionary(col);
    if (dict == null) {
      logger.warn("Dictionary for " + col + " was not found.");
    }
    dictionaryMap.put(col,cubeSegment.getDictionary(col));
  }
  DoggedCubeBuilder cubeBuilder=new DoggedCubeBuilder(cube.getDescriptor(),flatDesc,dictionaryMap);
  cubeBuilder.setReserveMemoryMB(calculateReserveMB(context.getConfiguration()));
  ExecutorService executorService=Executors.newSingleThreadExecutor();
  future=executorService.submit(cubeBuilder.buildAsRunnable(queue,new MapContextGTRecordWriter(context,cubeDesc,cubeSegment)));
}
