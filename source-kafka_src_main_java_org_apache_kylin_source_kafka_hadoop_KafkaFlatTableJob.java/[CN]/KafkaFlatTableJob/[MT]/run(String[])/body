{
  Options options=new Options();
  try {
    options.addOption(OPTION_JOB_NAME);
    options.addOption(OPTION_CUBE_NAME);
    options.addOption(OPTION_OUTPUT_PATH);
    options.addOption(OPTION_SEGMENT_ID);
    parseOptions(options,args);
    job=Job.getInstance(getConf(),getOptionValue(OPTION_JOB_NAME));
    String cubeName=getOptionValue(OPTION_CUBE_NAME);
    Path output=new Path(getOptionValue(OPTION_OUTPUT_PATH));
    String segmentId=getOptionValue(OPTION_SEGMENT_ID);
    CubeManager cubeMgr=CubeManager.getInstance(KylinConfig.getInstanceFromEnv());
    CubeInstance cube=cubeMgr.getCube(cubeName);
    job.getConfiguration().set(BatchConstants.CFG_CUBE_NAME,cubeName);
    job.getConfiguration().set(BatchConstants.CFG_CUBE_SEGMENT_ID,segmentId);
    logger.info("Starting: " + job.getJobName());
    setJobClasspath(job,cube.getConfig());
    KafkaConfigManager kafkaConfigManager=KafkaConfigManager.getInstance(KylinConfig.getInstanceFromEnv());
    KafkaConfig kafkaConfig=kafkaConfigManager.getKafkaConfig(cube.getFactTable());
    String brokers=KafkaClient.getKafkaBrokers(kafkaConfig);
    String topic=kafkaConfig.getTopic();
    if (brokers == null || brokers.length() == 0 || topic == null) {
      throw new IllegalArgumentException("Invalid Kafka information, brokers " + brokers + ", topic "+ topic);
    }
    job.getConfiguration().set(CONFIG_KAFKA_BROKERS,brokers);
    job.getConfiguration().set(CONFIG_KAFKA_TOPIC,topic);
    job.getConfiguration().set(CONFIG_KAFKA_TIMEOUT,String.valueOf(kafkaConfig.getTimeout()));
    job.getConfiguration().set(CONFIG_KAFKA_BUFFER_SIZE,String.valueOf(kafkaConfig.getBufferSize()));
    job.getConfiguration().set(CONFIG_KAFKA_INPUT_FORMAT,"json");
    job.getConfiguration().set(CONFIG_KAFKA_PARSER_NAME,kafkaConfig.getParserName());
    job.getConfiguration().set(CONFIG_KAFKA_CONSUMER_GROUP,cubeName);
    setupMapper(cube.getSegmentById(segmentId));
    job.setNumReduceTasks(0);
    FileOutputFormat.setOutputPath(job,output);
    FileOutputFormat.setCompressOutput(job,true);
    org.apache.log4j.Logger.getRootLogger().info("Output hdfs location: " + output);
    org.apache.log4j.Logger.getRootLogger().info("Output hdfs compression: " + true);
    job.getConfiguration().set(BatchConstants.CFG_OUTPUT_PATH,output.toString());
    deletePath(job.getConfiguration(),output);
    attachKylinPropsAndMetadata(cube,job.getConfiguration());
    return waitForCompletion(job);
  }
 catch (  Exception e) {
    logger.error("error in KafkaFlatTableJob",e);
    printUsage(options);
    throw e;
  }
 finally {
    if (job != null)     cleanupTempConfFile(job.getConfiguration());
  }
}
