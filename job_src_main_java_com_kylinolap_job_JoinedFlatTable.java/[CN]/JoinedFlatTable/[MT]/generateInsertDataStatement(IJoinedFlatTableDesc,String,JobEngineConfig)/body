{
  List<String> sqlList=Lists.newArrayList();
  File hadoopPropertiesFile=new File(engineConfig.getHadoopJobConfFilePath(intermediateTableDesc.getCapacity()));
  if (hadoopPropertiesFile.exists()) {
    DocumentBuilderFactory factory=DocumentBuilderFactory.newInstance();
    DocumentBuilder builder;
    Document doc;
    try {
      builder=factory.newDocumentBuilder();
      doc=builder.parse(hadoopPropertiesFile);
      NodeList nl=doc.getElementsByTagName("property");
      for (int i=0; i < nl.getLength(); i++) {
        String name=doc.getElementsByTagName("name").item(i).getFirstChild().getNodeValue();
        String value=doc.getElementsByTagName("value").item(i).getFirstChild().getNodeValue();
        if (name.equals("tmpjars") == false) {
          sqlList.add("SET " + name + "="+ value);
        }
      }
    }
 catch (    ParserConfigurationException e) {
      throw new IOException(e);
    }
catch (    SAXException e) {
      throw new IOException(e);
    }
  }
  sqlList.add("SET hive.exec.compress.output=true");
  sqlList.add("SET hive.auto.convert.join.noconditionaltask = true");
  sqlList.add("SET hive.auto.convert.join.noconditionaltask.size = 300000000");
  sqlList.add("INSERT OVERWRITE TABLE " + intermediateTableDesc.getTableName(jobUUID) + " "+ generateSelectDataStatement(intermediateTableDesc));
  return sqlList.toArray(new String[sqlList.size()]);
}
