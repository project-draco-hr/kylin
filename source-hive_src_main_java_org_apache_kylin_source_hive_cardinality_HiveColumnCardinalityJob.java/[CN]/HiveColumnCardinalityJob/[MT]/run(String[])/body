{
  Options options=new Options();
  try {
    options.addOption(OPTION_TABLE);
    options.addOption(OPTION_OUTPUT_PATH);
    parseOptions(options,args);
    String jobName=JOB_TITLE + getOptionsAsString();
    logger.info("Starting: " + jobName);
    Configuration conf=getConf();
    JobEngineConfig jobEngineConfig=new JobEngineConfig(KylinConfig.getInstanceFromEnv());
    conf.addResource(jobEngineConfig.getHadoopJobConfFilePath(null));
    job=Job.getInstance(conf,jobName);
    setJobClasspath(job);
    String table=getOptionValue(OPTION_TABLE);
    job.getConfiguration().set(BatchConstants.TABLE_NAME,table);
    Path output=new Path(getOptionValue(OPTION_OUTPUT_PATH));
    FileOutputFormat.setOutputPath(job,output);
    job.getConfiguration().set("dfs.block.size","67108864");
    IMRTableInputFormat tableInputFormat=MRUtil.getTableInputFormat(table);
    tableInputFormat.configureJob(job);
    job.setMapperClass(ColumnCardinalityMapper.class);
    job.setMapOutputKeyClass(IntWritable.class);
    job.setMapOutputValueClass(BytesWritable.class);
    job.setReducerClass(ColumnCardinalityReducer.class);
    job.setOutputFormatClass(TextOutputFormat.class);
    job.setOutputKeyClass(IntWritable.class);
    job.setOutputValueClass(LongWritable.class);
    job.setNumReduceTasks(1);
    this.deletePath(job.getConfiguration(),output);
    logger.info("Going to submit HiveColumnCardinalityJob for table '" + table + "'");
    TableDesc tableDesc=MetadataManager.getInstance(KylinConfig.getInstanceFromEnv()).getTableDesc(table);
    attachKylinPropsAndMetadata(tableDesc,job.getConfiguration());
    int result=waitForCompletion(job);
    return result;
  }
 catch (  Exception e) {
    printUsage(options);
    throw e;
  }
}
