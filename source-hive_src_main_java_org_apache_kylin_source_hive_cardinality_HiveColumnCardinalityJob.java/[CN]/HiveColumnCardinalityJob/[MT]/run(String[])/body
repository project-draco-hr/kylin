{
  Options options=new Options();
  options.addOption(OPTION_TABLE);
  options.addOption(OPTION_OUTPUT_PATH);
  parseOptions(options,args);
  String jobName=JOB_TITLE + getOptionsAsString();
  logger.info("Starting: " + jobName);
  Configuration conf=getConf();
  KylinConfig kylinConfig=KylinConfig.getInstanceFromEnv();
  JobEngineConfig jobEngineConfig=new JobEngineConfig(kylinConfig);
  conf.addResource(new Path(jobEngineConfig.getHadoopJobConfFilePath(null)));
  job=Job.getInstance(conf,jobName);
  setJobClasspath(job,kylinConfig);
  String table=getOptionValue(OPTION_TABLE);
  job.getConfiguration().set(BatchConstants.CFG_TABLE_NAME,table);
  Path output=new Path(getOptionValue(OPTION_OUTPUT_PATH));
  FileOutputFormat.setOutputPath(job,output);
  job.getConfiguration().set("dfs.block.size","67108864");
  IMRTableInputFormat tableInputFormat=MRUtil.getTableInputFormat(table);
  tableInputFormat.configureJob(job);
  job.setMapperClass(ColumnCardinalityMapper.class);
  job.setMapOutputKeyClass(IntWritable.class);
  job.setMapOutputValueClass(BytesWritable.class);
  job.setReducerClass(ColumnCardinalityReducer.class);
  job.setOutputFormatClass(TextOutputFormat.class);
  job.setOutputKeyClass(IntWritable.class);
  job.setOutputValueClass(LongWritable.class);
  job.setNumReduceTasks(1);
  this.deletePath(job.getConfiguration(),output);
  logger.info("Going to submit HiveColumnCardinalityJob for table '" + table + "'");
  TableDesc tableDesc=MetadataManager.getInstance(kylinConfig).getTableDesc(table);
  attachKylinPropsAndMetadata(tableDesc,job.getConfiguration());
  int result=waitForCompletion(job);
  return result;
}
