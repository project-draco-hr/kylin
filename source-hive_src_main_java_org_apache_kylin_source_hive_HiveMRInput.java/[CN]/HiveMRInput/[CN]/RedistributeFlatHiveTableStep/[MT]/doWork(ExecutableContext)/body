{
  KylinConfig config=getCubeSpecificConfig();
  try {
    computeRowCount(config.getCliCommandExecutor());
    Path rowCountFile=new Path(getRowCountOutputDir(),"000000_0");
    long rowCount=readRowCountFromFile(rowCountFile);
    if (!config.isEmptySegmentAllowed() && rowCount == 0) {
      stepLogger.log("Detect upstream hive table is empty, " + "fail the job because \"kylin.job.allow.empty.segment\" = \"false\"");
      return new ExecuteResult(ExecuteResult.State.ERROR,stepLogger.getBufferedLog());
    }
    int mapperInputRows=config.getHadoopJobMapperInputRows();
    int numReducers=Math.round(rowCount / ((float)mapperInputRows));
    numReducers=Math.max(1,numReducers);
    numReducers=Math.min(numReducers,config.getHadoopJobMaxReducerNumber());
    stepLogger.log("total input rows = " + rowCount);
    stepLogger.log("expected input rows per mapper = " + mapperInputRows);
    stepLogger.log("num reducers for RedistributeFlatHiveTableStep = " + numReducers);
    redistributeTable(config,numReducers);
    return new ExecuteResult(ExecuteResult.State.SUCCEED,stepLogger.getBufferedLog());
  }
 catch (  Exception e) {
    logger.error("job:" + getId() + " execute finished with exception",e);
    return new ExecuteResult(ExecuteResult.State.ERROR,stepLogger.getBufferedLog());
  }
}
