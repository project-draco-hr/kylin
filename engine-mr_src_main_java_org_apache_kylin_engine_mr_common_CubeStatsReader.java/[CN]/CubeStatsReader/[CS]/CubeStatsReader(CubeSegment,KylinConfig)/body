{
  ResourceStore store=ResourceStore.getStore(kylinConfig);
  String statsKey=cubeSegment.getStatisticsResourcePath();
  File tmpSeqFile=writeTmpSeqFile(store.getResource(statsKey).inputStream);
  Reader reader=null;
  try {
    Configuration hadoopConf=HadoopUtil.getCurrentConfiguration();
    Path path=new Path(HadoopUtil.fixWindowsPath("file://" + tmpSeqFile.getAbsolutePath()));
    Option seqInput=SequenceFile.Reader.file(path);
    reader=new SequenceFile.Reader(hadoopConf,seqInput);
    int percentage=100;
    double mapperOverlapRatio=0;
    Map<Long,HyperLogLogPlusCounter> counterMap=Maps.newHashMap();
    LongWritable key=(LongWritable)ReflectionUtils.newInstance(reader.getKeyClass(),hadoopConf);
    BytesWritable value=(BytesWritable)ReflectionUtils.newInstance(reader.getValueClass(),hadoopConf);
    while (reader.next(key,value)) {
      if (key.get() == 0L) {
        percentage=Bytes.toInt(value.getBytes());
      }
 else       if (key.get() == -1) {
        mapperOverlapRatio=Bytes.toDouble(value.getBytes());
      }
 else {
        HyperLogLogPlusCounter hll=new HyperLogLogPlusCounter(kylinConfig.getCubeStatsHLLPrecision());
        ByteArray byteArray=new ByteArray(value.getBytes());
        hll.readRegisters(byteArray.asBuffer());
        counterMap.put(key.get(),hll);
      }
    }
    this.seg=cubeSegment;
    this.samplingPercentage=percentage;
    this.mapperOverlapRatioOfFirstBuild=mapperOverlapRatio;
    this.cuboidRowEstimatesHLL=counterMap;
  }
  finally {
    IOUtils.closeStream(reader);
    tmpSeqFile.delete();
  }
}
