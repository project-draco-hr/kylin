{
  Options options=new Options();
  try {
    options.addOption(OPTION_TABLE);
    options.addOption(OPTION_OUTPUT_PATH);
    parseOptions(options,args);
    String jobName=JOB_TITLE + getOptionsAsString();
    System.out.println("Starting: " + jobName);
    Configuration conf=getConf();
    job=Job.getInstance(conf,jobName);
    if (jarPath == null || !new File(jarPath).exists()) {
      job.setJarByClass(getClass());
    }
 else {
      job.setJar(jarPath);
    }
    Path output=new Path(getOptionValue(OPTION_OUTPUT_PATH));
    FileOutputFormat.setOutputPath(job,output);
    job.getConfiguration().set("dfs.block.size","67108864");
    this.table=getOptionValue(OPTION_TABLE);
    String[] dbTableNames=HadoopUtil.parseHiveTableName(table);
    HCatInputFormat.setInput(job,dbTableNames[0],dbTableNames[1]);
    job.setInputFormatClass(HCatInputFormat.class);
    job.setMapperClass(ColumnCardinalityMapper.class);
    job.setMapOutputKeyClass(IntWritable.class);
    job.setMapOutputValueClass(BytesWritable.class);
    job.setReducerClass(ColumnCardinalityReducer.class);
    job.setOutputFormatClass(TextOutputFormat.class);
    job.setOutputKeyClass(IntWritable.class);
    job.setOutputValueClass(LongWritable.class);
    job.setNumReduceTasks(1);
    this.deletePath(job.getConfiguration(),output);
    System.out.println("Going to submit HiveColumnCardinalityJob for table '" + table + "'");
    int result=waitForCompletion(job);
    return result;
  }
 catch (  Exception e) {
    printUsage(options);
    e.printStackTrace(System.err);
    logger.error(e.getLocalizedMessage(),e);
    return 2;
  }
}
